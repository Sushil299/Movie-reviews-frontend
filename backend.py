# -*- coding: utf-8 -*-
"""backend

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TiA6-37RYxY1T4RB18XXJ009WhLO_oFm
"""

# -*- coding: utf-8 -*-
"""backend

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TiA6-37RYxY1T4RB18XXJ009WhLO_oFm
"""

import os
import asyncpraw
import google.generativeai as genai
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from typing import Dict, Any
import logging
from datetime import datetime, timedelta
import json
import asyncio
from asyncio import sleep

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load API keys from environment variables
REDDIT_CLIENT_ID = os.getenv("REDDIT_CLIENT_ID")
REDDIT_CLIENT_SECRET = os.getenv("REDDIT_CLIENT_SECRET")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Initialize APIs
reddit = asyncpraw.Reddit(
    client_id=REDDIT_CLIENT_ID,
    client_secret=REDDIT_CLIENT_SECRET,
    user_agent="StockScraper"
)

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

INDIAN_MOVIE_SUBREDDITS = [
    "bollywood", "IndianCinema", "tollywood", "kollywood", "MalayalamMovies",
    "Lollywood", "BollyBlindsNGossip", "bollywoodmemes", "India", "AskIndia",
    "movies", "moviecritic", "shittymoviedetails", "netflix", "boxoffice"
]

# FastAPI App
app = FastAPI()

# Add CORS middleware to allow requests from any origin (important for frontend access)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)

@app.get("/")
async def home():
    return {"message": "Movie Review Backend is Running!"}

@app.get("/search_reddit")
async def search_reddit(movie_name: str, days: int = 60) -> Dict[str, Any]:
    comments, posts = [], []
    total_posts = 0
    time_threshold = datetime.utcnow() - timedelta(days=days)

    for subreddit_name in INDIAN_MOVIE_SUBREDDITS:
        try:
            # Add debug logging for authentication check
            logger.info(f"Searching subreddit: {subreddit_name} for movie: {movie_name}")

            # Create subreddit instance
            subreddit = await reddit.subreddit(subreddit_name)

            # Create search generator
            search_generator = subreddit.search(movie_name, time_filter="month", limit=10)

            # Collect search results into a list to verify we have data
            search_results = []
            async for result in search_generator:
                search_results.append(result)

            # Check if we found any results
            if not search_results:
                logger.info(f"No results found for {movie_name} in {subreddit_name}")
                continue

            # Process the results
            for post in search_results:
                post_time = datetime.fromtimestamp(post.created_utc)
                if post_time < time_threshold or post.score < 50:
                    continue

                total_posts += 1
                posts.append({
                    "title": post.title,
                    "score": post.score,
                    "url": f"https://www.reddit.com{post.permalink}",
                    "num_comments": post.num_comments
                })

                # Make sure to handle comments asynchronously
                try:
                    await post.comments.replace_more(limit=5)
                    # Get comment list ONLY if post.comments is not None
                    if post.comments is None:
                        logger.warning(f"No comments object for post in {subreddit_name}")
                        continue

                    # Safely get comments list - handle possible None cases
                    try:
                        comment_list = post.comments.list()
                        if comment_list is None:
                            logger.warning(f"Comment list is None for post in {subreddit_name}")
                            continue
                    except Exception as list_err:
                        logger.warning(f"Error getting comment list for post in {subreddit_name}: {str(list_err)}")
                        continue

                    # Now process comments safely
                    for comment in comment_list[:20]:
                        # Check if comment is valid and has required attributes
                        if (comment is not None and
                            hasattr(comment, 'body') and
                            hasattr(comment, 'score') and
                            hasattr(comment, 'author') and
                            hasattr(comment, 'permalink')):

                            if len(comment.body.strip()) > 30 and comment.score >= 20:
                                comments.append({
                                    "text": comment.body,
                                    "score": comment.score,
                                    "author": str(comment.author),
                                    "url": f"https://www.reddit.com{comment.permalink}"
                                })
                except Exception as comment_err:
                    logger.warning(f"Error processing comments for post in {subreddit_name}: {str(comment_err)}")
                    continue

        except Exception as e:
            logger.warning(f"Error searching subreddit {subreddit_name}: {str(e)}")
            # Add a delay between requests to avoid rate limiting
            await sleep(1)
            continue

        # Add a small delay between subreddit searches to avoid rate limiting
        await sleep(0.5)

    # Sort comments by score
    comments.sort(key=lambda x: x["score"], reverse=True)

    # Log the results for debugging
    logger.info(f"Found {len(posts)} posts and {len(comments)} comments for {movie_name}")

    return {"posts": posts, "comments": comments[:50], "total_posts": total_posts}

@app.get("/analyze")
async def analyze_with_gemini(movie_name: str) -> Dict[str, Any]:
    try:
        # Get Reddit data
        reddit_data = await search_reddit(movie_name, days=60)

        # Log the data for debugging
        logger.info(f"Reddit data for {movie_name}: {len(reddit_data['posts'])} posts, {len(reddit_data['comments'])} comments")

        # Check if we have enough data to analyze
        if not reddit_data["posts"] and not reddit_data["comments"]:
            logger.warning(f"Insufficient data found for movie: {movie_name}")
            return {
                "error": "insufficient_data",
                "message": f"Not enough Reddit discussions found about {movie_name}",
                "tldr_summary": f"There isn't enough online discussion available to form an analysis of {movie_name}."
            }

        # Prepare text for analysis
        posts_text = "\n\n".join([f"Post: {p['title']}" for p in reddit_data["posts"]])
        comments_text = "\n\n".join([f"Comment: {c['text']}" for c in reddit_data["comments"][:30]])

        prompt = f"""
        Based on Reddit discussions and comments about {movie_name} in posts {posts_text} and comments {comments_text}, provide a detailed analysis to help users decide whether to watch the movie.

        If limited data is available (fewer than 10 meaningful comments/posts), please note this limitation at the beginning of your response.

        Include:

        1. **TL;DR Summary**
          - A concise 1-2 sentence verdict on the movie.

        2. **Overall Sentiment Analysis**
          - Percentage breakdown of Positive, Negative, and Neutral opinions.
          - Include 3-5 key phrases/words that drove your sentiment classification.
          - Note confidence level of sentiment analysis (high/medium/low).

        3. **Summary of Audience Reactions**
          - A 5-7 sentence overview of common praises and criticisms.
          - Indicate if opinions are polarized or generally consistent.
          - Note if analysis contains potential spoilers.

        4. **Key Aspects Discussed**
          - Rating and analysis for: Acting, Story, Direction, Music, Cinematography, and Special Effects.
          - For each aspect, provide a score (1-10) and 1-2 sentence explanation.

        5. **Common Praise & Complaints**
          - 3-5 most frequently mentioned positive aspects.
          - 3-5 most frequently mentioned criticisms.

        6. **Comparison with Similar Movies**
          - List 3-5 similar Indian movies that viewers might enjoy.
          - For each recommendation, provide:
            * Title
            * Year
            * Brief explanation of similarity (theme, director, style, actors)
            * Whether it's rated better/worse than the movie in question

        7. **Final Verdict**
          - Who would most likely enjoy this movie? (demographics, interests, preferences)
          - Who might not enjoy it?
          - Is it worth watching in theaters or better for streaming?

        Respond in a structured **JSON format**.
        """

        # Generate analysis with Gemini
        logger.info(f"Sending request to Gemini for movie: {movie_name}")
        response = model.generate_content(prompt)

        # Parse the response
        try:
            response_text = response.text

            # Log the raw response for debugging
            logger.info(f"Raw Gemini response first 200 chars: {response_text[:200]}...")

            # Check if the response contains JSON
            if "```json" in response_text:
                analysis = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                # Try to extract any code block
                analysis = response_text.split("```")[1].split("```")[0].strip()
            else:
                # Try to extract JSON directly
                analysis = response_text.strip()

            # Parse JSON
            try:
                analysis_json = json.loads(analysis)
            except json.JSONDecodeError:
                # If that fails, try to find anything that looks like JSON
                import re
                json_pattern = r'\{.*\}'
                match = re.search(json_pattern, response_text, re.DOTALL)
                if match:
                    try:
                        analysis_json = json.loads(match.group(0))
                    except:
                        raise Exception("Could not parse JSON from response")
                else:
                    raise Exception("No JSON-like structure found in response")

            logger.info(f"Successfully parsed Gemini response for {movie_name}")
            return analysis_json

        except Exception as parse_err:
            logger.error(f"Error parsing Gemini response: {str(parse_err)}")
            logger.error(f"Raw response: {response.text[:500]}...")  # Log first 500 chars

            # Return a fallback response
            return {
                "error": "parsing_error",
                "message": "Error parsing the AI response",
                "tldr_summary": f"Unable to generate a proper analysis for {movie_name} due to technical issues."
            }

    except Exception as e:
        logger.error(f"Error in analyze_with_gemini: {str(e)}")
        return {
            "error": "general_error",
            "message": str(e),
            "tldr_summary": f"An error occurred while analyzing {movie_name}."
        }

# Run the app using `uvicorn backend:app --host 0.0.0.0 --port 8000`