# -*- coding: utf-8 -*-
"""backend

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TiA6-37RYxY1T4RB18XXJ009WhLO_oFm
"""

import os
import asyncpraw
import google.generativeai as genai
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from typing import Dict, Any
import logging
from datetime import datetime, timedelta
import asyncio
from asyncio import sleep

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load API keys from environment variables
REDDIT_CLIENT_ID = os.getenv("REDDIT_CLIENT_ID")
REDDIT_CLIENT_SECRET = os.getenv("REDDIT_CLIENT_SECRET")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Initialize APIs
reddit = asyncpraw.Reddit(
    client_id=REDDIT_CLIENT_ID,
    client_secret=REDDIT_CLIENT_SECRET,
    user_agent="MovieReviewScraper"
)

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

MOVIE_SUBREDDITS = [
    "bollywood", "IndianCinema", "tollywood", "kollywood", "MalayalamMovies",
    "movies", "boxoffice", "flicks", "truefilm"
]

# FastAPI App
app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def home():
    return {"message": "Movie Review Backend is Running!"}

@app.get("/search_reddit")
async def search_reddit(movie_name: str, days: int = 60) -> Dict[str, Any]:
    comments, posts = [], []
    total_posts = 0
    time_threshold = datetime.utcnow() - timedelta(days=days)

    for subreddit_name in MOVIE_SUBREDDITS:
        try:
            logger.info(f"Searching subreddit: {subreddit_name} for movie: {movie_name}")

            # Create subreddit instance
            subreddit = await reddit.subreddit(subreddit_name)

            # Create search generator
            search_generator = subreddit.search(movie_name, time_filter="month", limit=10)

            # Collect search results
            search_results = []
            async for result in search_generator:
                search_results.append(result)

            # Check if we found any results
            if not search_results:
                logger.info(f"No results found for {movie_name} in {subreddit_name}")
                continue

            # Process the results
            for post in search_results:
                post_time = datetime.fromtimestamp(post.created_utc)
                if post_time < time_threshold or post.score < 50:
                    continue

                total_posts += 1
                posts.append({
                    "title": post.title,
                    "score": post.score,
                    "url": f"https://www.reddit.com{post.permalink}",
                    "num_comments": post.num_comments
                })

                # Handle comments
                try:
                    await post.comments.replace_more(limit=5)
                    if post.comments is None:
                        continue

                    try:
                        comment_list = post.comments.list()
                        if comment_list is None:
                            continue
                    except Exception as list_err:
                        logger.warning(f"Error getting comment list: {str(list_err)}")
                        continue

                    for comment in comment_list[:20]:
                        if (comment is not None and
                            hasattr(comment, 'body') and
                            hasattr(comment, 'score') and
                            hasattr(comment, 'author') and
                            hasattr(comment, 'permalink')):

                            if len(comment.body.strip()) > 30 and comment.score >= 20:
                                comments.append({
                                    "text": comment.body,
                                    "score": comment.score,
                                    "author": str(comment.author),
                                    "url": f"https://www.reddit.com{comment.permalink}"
                                })
                except Exception as comment_err:
                    logger.warning(f"Error processing comments: {str(comment_err)}")
                    continue

        except Exception as e:
            logger.warning(f"Error searching subreddit {subreddit_name}: {str(e)}")
            await sleep(1)
            continue

        await sleep(0.5)

    # Sort comments by score
    comments.sort(key=lambda x: x["score"], reverse=True)

    logger.info(f"Found {len(posts)} posts and {len(comments)} comments for {movie_name}")

    return {"posts": posts, "comments": comments[:50], "total_posts": total_posts}

@app.get("/analyze")
async def analyze_with_gemini(movie_name: str) -> Dict[str, Any]:
    try:
        # Get Reddit data
        reddit_data = await search_reddit(movie_name, days=60)

        logger.info(f"Reddit data for {movie_name}: {len(reddit_data['posts'])} posts, {len(reddit_data['comments'])} comments")

        # Check if we have enough data to analyze
        if not reddit_data["posts"] and not reddit_data["comments"]:
            logger.warning(f"Insufficient data found for movie: {movie_name}")
            return {
                "title": f"{movie_name} Analysis (Insufficient Data)",
                "sections": [
                    {"title": "TL;DR Summary", "content": f"There isn't enough online discussion available to form an analysis of {movie_name}."},
                    {"title": "Sentiment", "content": "No data available", "positive": 0, "negative": 0, "neutral": 0},
                    {"title": "Audience Reactions", "content": "Insufficient data to summarize audience reactions."},
                    {"title": "Key Aspects", "content": "Insufficient data", "items": []},
                    {"title": "Praise & Complaints", "content": "Insufficient data", "praise": [], "complaints": []},
                    {"title": "Similar Movies", "content": "Insufficient data", "movies": []},
                    {"title": "Final Verdict", "content": "Insufficient data"}
                ]
            }

        # Prepare text for analysis
        posts_text = "\n\n".join([f"Post: {p['title']}" for p in reddit_data["posts"]])
        comments_text = "\n\n".join([f"Comment: {c['text']}" for c in reddit_data["comments"][:30]])

        # Create a simpler prompt that requests structured text, not JSON
        prompt = f"""
        Analyze the sentiment and key aspects of the movie "{movie_name}" based on these Reddit discussions.

        POSTS:
        {posts_text}

        COMMENTS:
        {comments_text}

        IMPORTANT: Format your response as plain text with these section headings:

        === TL;DR SUMMARY ===
        (Write a brief 2-3 sentence summary of your findings)

        === SENTIMENT ANALYSIS ===
        Positive: (percentage as a number)
        Negative: (percentage as a number)
        Neutral: (percentage as a number)

        Key Phrases:
        - (phrase 1)
        - (phrase 2)
        - (phrase 3)
        - (phrase 4)
        - (phrase 5)

        === AUDIENCE REACTIONS ===
        (Write a paragraph summarizing audience reactions)

        === KEY ASPECTS ===
        Acting: (score from 1-10) - (explanation)
        Story: (score from 1-10) - (explanation)
        Direction: (score from 1-10) - (explanation)
        Music: (score from 1-10) - (explanation)
        Cinematography: (score from 1-10) - (explanation)

        === PRAISE & COMPLAINTS ===
        Top Praise:
        - (praise 1)
        - (praise 2)
        - (praise 3)
        - (praise 4)
        - (praise 5)

        Top Complaints:
        - (complaint 1)
        - (complaint 2)
        - (complaint 3)
        - (complaint 4)
        - (complaint 5)

        === SIMILAR MOVIES ===
        - (movie 1) (year) - (brief similarity note)
        - (movie 2) (year) - (brief similarity note)
        - (movie 3) (year) - (brief similarity note)

        === FINAL VERDICT ===
        Who Would Enjoy: (brief description)
        Who Might Not Enjoy: (brief description)
        Theater or Streaming: (recommendation)
        """

        # Generate analysis with Gemini
        logger.info(f"Sending request to Gemini for movie: {movie_name}")
        generation_config = {
            "temperature": 0.1,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 4096,
        }

        safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            }
        ]

        response = model.generate_content(
            prompt,
            generation_config=generation_config,
            safety_settings=safety_settings
        )

        # Get plain text response
        response_text = response.text

        # Parse the plaintext response into structured data for the frontend
        # Split by sections
        sections = response_text.split("===")

        # Remove empty sections
        sections = [s.strip() for s in sections if s.strip()]

        # Parse the sections into a structure
        parsed_data = {"title": f"{movie_name} Analysis", "sections": []}

        # Initial values for sentiment
        positive = 0
        negative = 0
        neutral = 0
        key_phrases = []

        # Aspects list
        aspects = []

        # Praise and complaints
        praise = []
        complaints = []

        # Similar movies
        similar_movies = []

        # Process each section
        for section in sections:
            lines = section.strip().split("\n")
            section_title = lines[0].strip()
            section_content = "\n".join(lines[1:]).strip()

            # Create a basic section object
            section_obj = {
                "title": section_title,
                "content": section_content
            }

            # Extract special data for certain sections
            if "SENTIMENT ANALYSIS" in section_title:
                # Extract sentiment percentages
                for line in section_content.split("\n"):
                    if "Positive:" in line:
                        try:
                            positive = int(line.split("Positive:")[1].strip().split()[0].replace("%", ""))
                        except:
                            positive = 0
                    elif "Negative:" in line:
                        try:
                            negative = int(line.split("Negative:")[1].strip().split()[0].replace("%", ""))
                        except:
                            negative = 0
                    elif "Neutral:" in line:
                        try:
                            neutral = int(line.split("Neutral:")[1].strip().split()[0].replace("%", ""))
                        except:
                            neutral = 0

                # Extract key phrases
                if "Key Phrases:" in section_content:
                    phrases_section = section_content.split("Key Phrases:")[1].strip()
                    for line in phrases_section.split("\n"):
                        if line.strip().startswith("-"):
                            phrase = line.strip()[1:].strip()
                            if phrase:
                                key_phrases.append(phrase)

                section_obj["positive"] = positive
                section_obj["negative"] = negative
                section_obj["neutral"] = neutral
                section_obj["key_phrases"] = key_phrases

            elif "KEY ASPECTS" in section_title:
                # Extract aspect ratings and explanations
                for line in section_content.split("\n"):
                    if ":" in line and "-" in line:
                        aspect_name = line.split(":")[0].strip()
                        try:
                            score_part = line.split(":")[1].split("-")[0].strip()
                            explanation = line.split("-", 1)[1].strip()
                            score = int(score_part) if score_part.isdigit() else "N/A"
                            aspects.append({
                                "name": aspect_name,
                                "score": score,
                                "explanation": explanation
                            })
                        except:
                            pass

                section_obj["items"] = aspects

            elif "PRAISE & COMPLAINTS" in section_title:
                # Extract praise items
                if "Top Praise:" in section_content:
                    praise_section = section_content.split("Top Praise:")[1]
                    if "Top Complaints:" in praise_section:
                        praise_section = praise_section.split("Top Complaints:")[0]

                    for line in praise_section.split("\n"):
                        if line.strip().startswith("-"):
                            item = line.strip()[1:].strip()
                            if item:
                                praise.append(item)

                # Extract complaint items
                if "Top Complaints:" in section_content:
                    complaints_section = section_content.split("Top Complaints:")[1]
                    for line in complaints_section.split("\n"):
                        if line.strip().startswith("-"):
                            item = line.strip()[1:].strip()
                            if item:
                                complaints.append(item)

                section_obj["praise"] = praise
                section_obj["complaints"] = complaints

            elif "SIMILAR MOVIES" in section_title:
                # Extract similar movies
                for line in section_content.split("\n"):
                    if line.strip().startswith("-"):
                        movie_info = line.strip()[1:].strip()
                        if movie_info:
                            similar_movies.append(movie_info)

                section_obj["movies"] = similar_movies

            # Add the section to parsed data
            parsed_data["sections"].append(section_obj)

        return parsed_data

    except Exception as e:
        logger.error(f"Error in analyze_with_gemini: {str(e)}")
        return {
            "title": f"{movie_name} Analysis (Error)",
            "sections": [
                {"title": "Error", "content": f"An error occurred while analyzing {movie_name}. Please try again later."}
            ]
        }